{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Models\n",
    "<div style=\"color:red; font-size:14px;\">!! Don't define functions here, import them from utils.py</div>\n",
    "\n",
    "This notebook contains the code needed to train and store models to disk.\n",
    "\n",
    "Remember that if you use a function with a random state you have to fix it to a number so that the results are reproducible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\r\n"
     ]
    }
   ],
   "source": [
    "# Cython import\n",
    "!python skseq/setup.py build_ext --build-lib=./skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from skseq.id_feature import IDFeatures\n",
    "from skseq.extended_feature import ExtendedFeatures\n",
    "\n",
    "from skseq import structured_perceptron_c \n",
    "from skseq.structured_perceptron import StructuredPerceptron\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 38366/38366 [00:38<00:00, 1002.58sentence/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data_target_sets(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "We need to create our corpus using the training data. The corpus consists of two dictionaries, one for the words and one for the tags. The words dictionary maps each word to an index and the tags dictionary maps each tag to an index. We also need to create the reverse mapping for the tags dictionary. This is needed to convert the predictions back to the tag names.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "        sentences = [['I', 'love', 'Python'], ['Python', 'is', 'great']]\n",
    "        tags = ['O', 'O', 'B']\n",
    "        word_dict, tag_dict, tag_dict_rev = create_corpus(sentences, tags)\n",
    "        # word_dict: {'I': 0, 'love': 1, 'Python': 2, 'is': 3, 'great': 4}\n",
    "        # tag_dict: {'O': 0, 'B': 1}\n",
    "        # tag_dict_rev: {0: 'O', 1: 'B'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict, tag_dict, tag_dict_rev = create_corpus(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Training Sequence List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:30<00:00, 116.00sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_list(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:31<00:00, 115.68sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_listC(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 1/0 2/0 3/0 4/0 5/0 6/1 7/0 8/0 9/0 10/0 11/0 12/1 13/0 14/0 9/0 15/0 1/0 16/2 17/0 18/0 19/0 20/0 21/0 \n",
      "Thousands/O of/O demonstrators/O have/O marched/O through/O London/B-geo to/O protest/O the/O war/O in/O Iraq/B-geo and/O demand/O the/O withdrawal/O of/O British/B-gpe troops/O from/O that/O country/O ./O \n"
     ]
    }
   ],
   "source": [
    "print(train_seq[0])\n",
    "print(train_seq[0].to_words(sequence_list=train_seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ Default Features</h3>\n",
    "</div>\n",
    "\n",
    "To train the structured perceptron we must create a feature mapper and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[21] prev_tag:O::B-gpe\n",
      "[23] prev_tag:B-gpe::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[28] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[1] id:Thousands::O\n",
      "[2] id:of::O\n",
      "[4] id:demonstrators::O\n",
      "[5] id:have::O\n",
      "[6] id:marched::O\n",
      "[7] id:through::O\n",
      "[8] id:London::B-geo\n",
      "[10] id:to::O\n",
      "[12] id:protest::O\n",
      "[13] id:the::O\n",
      "[14] id:war::O\n",
      "[15] id:in::O\n",
      "[16] id:Iraq::B-geo\n",
      "[17] id:and::O\n",
      "[18] id:demand::O\n",
      "[13] id:the::O\n",
      "[19] id:withdrawal::O\n",
      "[2] id:of::O\n",
      "[20] id:British::B-gpe\n",
      "[22] id:troops::O\n",
      "[24] id:from::O\n",
      "[25] id:that::O\n",
      "[26] id:country::O\n",
      "[27] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper, train_seq[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp = StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "Epoch: 1 Accuracy: 0.931674\n",
      "Epoch: 2 Accuracy: 0.940913\n",
      "Epoch: 3 Accuracy: 0.946175\n",
      "Epoch: 4 Accuracy: 0.950018\n",
      "Epoch: 5 Accuracy: 0.952577\n",
      "Epoch: 6 Accuracy: 0.954425\n",
      "Epoch: 7 Accuracy: 0.956033\n",
      "Epoch: 8 Accuracy: 0.957185\n",
      "Epoch: 9 Accuracy: 0.958481\n",
      "Epoch: 10 Accuracy: 0.959217\n",
      "Epoch: 11 Accuracy: 0.960524\n",
      "Epoch: 12 Accuracy: 0.961121\n",
      "Epoch: 13 Accuracy: 0.961207\n",
      "Epoch: 14 Accuracy: 0.961983\n",
      "CPU times: user 1h 33min 59s, sys: 20 s, total: 1h 34min 19s\n",
      "Wall time: 1h 34min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_c = structured_perceptron_c.StructuredPerceptronC(word_dict, tag_dict, feature_mapper)\n",
    "sp_c.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "Epoch: 1 Accuracy: 0.931674\n",
      "Epoch: 2 Accuracy: 0.940913\n",
      "Epoch: 3 Accuracy: 0.946175\n",
      "Epoch: 4 Accuracy: 0.950018\n",
      "Epoch: 5 Accuracy: 0.952577\n",
      "Epoch: 6 Accuracy: 0.954425\n",
      "Epoch: 7 Accuracy: 0.956033\n",
      "Epoch: 8 Accuracy: 0.957185\n",
      "Epoch: 9 Accuracy: 0.958481\n",
      "Epoch: 10 Accuracy: 0.959217\n",
      "Epoch: 11 Accuracy: 0.960524\n",
      "Epoch: 12 Accuracy: 0.961121\n",
      "Epoch: 13 Accuracy: 0.961207\n",
      "Epoch: 14 Accuracy: 0.961983\n",
      "CPU times: user 59min 8s, sys: 13.5 s, total: 59min 21s\n",
      "Wall time: 59min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp_c.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp.save_model(\"fitted_models/01_SP_Default_Features\")\n",
    "sp_c.save_model(\"fitted_models/01C_SP_Default_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ New Features</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper_ext = ExtendedFeatures(train_seq)\n",
    "feature_mapper_ext.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[8] prev_tag:O::O\n",
      "[45] prev_tag:O::B-tim\n",
      "[47] prev_tag:B-tim::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[16] prev_tag:O::B-geo\n",
      "[18] prev_tag:B-geo::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[59] prev_tag:O::B-org\n",
      "[61] prev_tag:B-org::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[8] prev_tag:O::O\n",
      "[16] prev_tag:O::B-geo\n",
      "[77] prev_tag:B-geo::I-geo\n",
      "[78] prev_tag:I-geo::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[38] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[39, 2, 3] id:Helicopter::O\n",
      "[39, 2, 3] firstupper::O\n",
      "[39, 2, 3] alphanum::O\n",
      "[40, 5, 3] id:gunships::O\n",
      "[40, 5, 3] lower::O\n",
      "[40, 5, 3] alphanum::O\n",
      "[41, 42, 43, 44, 44] id:Saturday::B-tim\n",
      "[41, 42, 43, 44, 44] firstupper::B-tim\n",
      "[41, 42, 43, 44, 44] alphanum::B-tim\n",
      "[41, 42, 43, 44, 44] suffixday::B-tim\n",
      "[41, 42, 43, 44, 44] suffixday::B-tim\n",
      "[46, 5, 3] id:pounded::O\n",
      "[46, 5, 3] lower::O\n",
      "[46, 5, 3] alphanum::O\n",
      "[48, 5, 3] id:militant::O\n",
      "[48, 5, 3] lower::O\n",
      "[48, 5, 3] alphanum::O\n",
      "[49, 5, 3] id:hideouts::O\n",
      "[49, 5, 3] lower::O\n",
      "[49, 5, 3] alphanum::O\n",
      "[22, 5, 3, 6, 7] id:in::O\n",
      "[22, 5, 3, 6, 7] lower::O\n",
      "[22, 5, 3, 6, 7] alphanum::O\n",
      "[22, 5, 3, 6, 7] preposition::O\n",
      "[22, 5, 3, 6, 7] stopword::O\n",
      "[20, 5, 3, 6, 7] id:the::O\n",
      "[20, 5, 3, 6, 7] lower::O\n",
      "[20, 5, 3, 6, 7] alphanum::O\n",
      "[20, 5, 3, 6, 7] preposition::O\n",
      "[20, 5, 3, 6, 7] stopword::O\n",
      "[50, 14, 15] id:Orakzai::B-geo\n",
      "[50, 14, 15] firstupper::B-geo\n",
      "[50, 14, 15] alphanum::B-geo\n",
      "[51, 5, 3] id:tribal::O\n",
      "[51, 5, 3] lower::O\n",
      "[51, 5, 3] alphanum::O\n",
      "[52, 5, 3] id:region::O\n",
      "[52, 5, 3] lower::O\n",
      "[52, 5, 3] alphanum::O\n",
      "[53] id:,::O\n",
      "[54, 5, 3, 7] id:where::O\n",
      "[54, 5, 3, 7] lower::O\n",
      "[54, 5, 3, 7] alphanum::O\n",
      "[54, 5, 3, 7] stopword::O\n",
      "[55, 5, 3] id:many::O\n",
      "[55, 5, 3] lower::O\n",
      "[55, 5, 3] alphanum::O\n",
      "[56, 57, 58] id:Taliban::B-org\n",
      "[56, 57, 58] firstupper::B-org\n",
      "[56, 57, 58] alphanum::B-org\n",
      "[60, 5, 3] id:militants::O\n",
      "[60, 5, 3] lower::O\n",
      "[60, 5, 3] alphanum::O\n",
      "[62, 5, 3, 7] id:are::O\n",
      "[62, 5, 3, 7] lower::O\n",
      "[62, 5, 3, 7] alphanum::O\n",
      "[62, 5, 3, 7] stopword::O\n",
      "[63, 5, 3] id:believed::O\n",
      "[63, 5, 3] lower::O\n",
      "[63, 5, 3] alphanum::O\n",
      "[17, 5, 3, 6, 7] id:to::O\n",
      "[17, 5, 3, 6, 7] lower::O\n",
      "[17, 5, 3, 6, 7] alphanum::O\n",
      "[17, 5, 3, 6, 7] preposition::O\n",
      "[17, 5, 3, 6, 7] stopword::O\n",
      "[10, 5, 3, 7] id:have::O\n",
      "[10, 5, 3, 7] lower::O\n",
      "[10, 5, 3, 7] alphanum::O\n",
      "[10, 5, 3, 7] stopword::O\n",
      "[64, 5, 3] id:fled::O\n",
      "[64, 5, 3] lower::O\n",
      "[64, 5, 3] alphanum::O\n",
      "[17, 5, 3, 6, 7] id:to::O\n",
      "[17, 5, 3, 6, 7] lower::O\n",
      "[17, 5, 3, 6, 7] alphanum::O\n",
      "[17, 5, 3, 6, 7] preposition::O\n",
      "[17, 5, 3, 6, 7] stopword::O\n",
      "[65, 5, 3] id:avoid::O\n",
      "[65, 5, 3] lower::O\n",
      "[65, 5, 3] alphanum::O\n",
      "[66, 5, 3, 7] id:an::O\n",
      "[66, 5, 3, 7] lower::O\n",
      "[66, 5, 3, 7] alphanum::O\n",
      "[66, 5, 3, 7] stopword::O\n",
      "[67, 5, 3] id:earlier::O\n",
      "[67, 5, 3] lower::O\n",
      "[67, 5, 3] alphanum::O\n",
      "[68, 5, 3] id:military::O\n",
      "[68, 5, 3] lower::O\n",
      "[68, 5, 3] alphanum::O\n",
      "[69, 5, 3] id:offensive::O\n",
      "[69, 5, 3] lower::O\n",
      "[69, 5, 3] alphanum::O\n",
      "[22, 5, 3, 6, 7] id:in::O\n",
      "[22, 5, 3, 6, 7] lower::O\n",
      "[22, 5, 3, 6, 7] alphanum::O\n",
      "[22, 5, 3, 6, 7] preposition::O\n",
      "[22, 5, 3, 6, 7] stopword::O\n",
      "[70, 5, 3] id:nearby::O\n",
      "[70, 5, 3] lower::O\n",
      "[70, 5, 3] alphanum::O\n",
      "[71, 14, 15, 72] id:South::B-geo\n",
      "[71, 14, 15, 72] firstupper::B-geo\n",
      "[71, 14, 15, 72] alphanum::B-geo\n",
      "[71, 14, 15, 72] prefixSouth::B-geo\n",
      "[73, 74, 75, 76, 76] id:Waziristan::I-geo\n",
      "[73, 74, 75, 76, 76] firstupper::I-geo\n",
      "[73, 74, 75, 76, 76] alphanum::I-geo\n",
      "[73, 74, 75, 76, 76] suffixstan::I-geo\n",
      "[73, 74, 75, 76, 76] suffixstan::I-geo\n",
      "[37] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper_ext, train_seq[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_ext = StructuredPerceptron(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.930157\n",
      "Epoch: 1 Accuracy: 0.944669\n",
      "Epoch: 2 Accuracy: 0.948542\n",
      "Epoch: 3 Accuracy: 0.951180\n",
      "Epoch: 4 Accuracy: 0.953505\n",
      "Epoch: 5 Accuracy: 0.954438\n",
      "Epoch: 6 Accuracy: 0.955883\n",
      "Epoch: 7 Accuracy: 0.956796\n",
      "Epoch: 8 Accuracy: 0.957754\n",
      "Epoch: 9 Accuracy: 0.958079\n",
      "Epoch: 10 Accuracy: 0.959030\n",
      "Epoch: 11 Accuracy: 0.959852\n",
      "Epoch: 12 Accuracy: 0.959726\n",
      "Epoch: 13 Accuracy: 0.960705\n",
      "Epoch: 14 Accuracy: 0.961026\n",
      "CPU times: user 1h 3min 28s, sys: 16.2 s, total: 1h 3min 44s\n",
      "Wall time: 1h 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp_ext.fit(feature_mapper_ext.dataset, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_ext_c = structured_perceptron_c.StructuredPerceptronC(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext_c.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.930157\n",
      "Epoch: 1 Accuracy: 0.944669\n",
      "Epoch: 2 Accuracy: 0.948542\n",
      "Epoch: 3 Accuracy: 0.951180\n",
      "Epoch: 4 Accuracy: 0.953505\n",
      "Epoch: 5 Accuracy: 0.954438\n",
      "Epoch: 6 Accuracy: 0.955883\n",
      "Epoch: 7 Accuracy: 0.956796\n",
      "Epoch: 8 Accuracy: 0.957754\n",
      "Epoch: 9 Accuracy: 0.958079\n",
      "Epoch: 10 Accuracy: 0.959030\n",
      "Epoch: 11 Accuracy: 0.959852\n",
      "Epoch: 12 Accuracy: 0.959726\n",
      "Epoch: 13 Accuracy: 0.960705\n",
      "Epoch: 14 Accuracy: 0.961026\n",
      "CPU times: user 1h 40s, sys: 12.1 s, total: 1h 52s\n",
      "Wall time: 1h 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp_ext_c.fit(feature_mapper_ext.dataset, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_ext.save_model(\"fitted_models/02_SP_Extended_Features\")\n",
    "sp_ext_c.save_model(\"fitted_models/02C_SP_Extended_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3> BERT model. DL approach</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upload train set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_BERT = pd.read_csv(\"data/train_data_ner.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, Tokenizing and Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tag, enc_tag = process_BERT_data(train_BERT)\n",
    "X_train = sentences\n",
    "y_train = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38366/38366 [00:08<00:00, 4402.22it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids,attention_mask = tokenize_BERT(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Padding and Truncation Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{128}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: Checking Padding and Truncation length's\n",
    "was = list()\n",
    "for i in range(len(input_ids)):\n",
    "    was.append(len(input_ids[i]))\n",
    "set(was)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{128}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Padding\n",
    "train_tag = list()\n",
    "for i in range(len(y_train)):\n",
    "    train_tag.append(np.array(y_train[i] + [0] * (128-len(y_train[i]))))\n",
    "    \n",
    "# TEST:  Checking Padding Length\n",
    "was = list()\n",
    "for i in range(len(train_tag)):\n",
    "    was.append(len(train_tag[i]))\n",
    "set(was)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building BERT Model: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 09:53:47.322162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/opt/anaconda3/envs/master/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "model = create_BERT_model(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128, 17)      13073       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,495,313\n",
      "Trainable params: 109,495,313\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  1/640 [..............................] - ETA: 7:40:52 - loss: 3.1836 - accuracy: 0.0382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/nc/wbbw2w1x72dg89nnx4p_1xwr0000gn/T/ipykernel_3231/3371486011.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mearly_stopping\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'min'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m history_bert = model.fit([input_ids,attention_mask],\n\u001B[0m\u001B[1;32m      3\u001B[0m                          \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_tag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                          \u001B[0mepochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m25\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m30\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                          callbacks = early_stopping,verbose = True)\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1562\u001B[0m                         ):\n\u001B[1;32m   1563\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1564\u001B[0;31m                             \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1565\u001B[0m                             \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1566\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    913\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    914\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 915\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    916\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    945\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 947\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    948\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    949\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2494\u001B[0m       (graph_function,\n\u001B[1;32m   2495\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2496\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   2497\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   2498\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1860\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1861\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1862\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1863\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1864\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    497\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    498\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 499\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    500\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    501\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/master/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     55\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(mode='min',patience=5)\n",
    "history_bert = model.fit([input_ids,attention_mask],\n",
    "                         np.array(train_tag),\n",
    "                         epochs = 25,batch_size = 30*2,\n",
    "                         callbacks = early_stopping,verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"fitted_models/bert_model.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
