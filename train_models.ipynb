{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Models\n",
    "<div style=\"color:red; font-size:14px;\">!! Don't define functions here, import them from utils.py</div>\n",
    "\n",
    "This notebook contains the code needed to train and store models to disk.\n",
    "\n",
    "Remember that if you use a function with a random state you have to fix it to a number so that the results are reproducible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\r\n"
     ]
    }
   ],
   "source": [
    "# Cython import\n",
    "!python skseq/setup.py build_ext --build-lib=./skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from skseq.id_feature import IDFeatures\n",
    "from skseq.extended_feature import ExtendedFeatures\n",
    "\n",
    "from skseq import structured_perceptron_c \n",
    "from skseq.structured_perceptron import StructuredPerceptron\n",
    "\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 38366/38366 [00:41<00:00, 933.59sentence/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data_target_sets(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "We need to create our corpus using the training data. The corpus consists of two dictionaries, one for the words and one for the tags. The words dictionary maps each word to an index and the tags dictionary maps each tag to an index. We also need to create the reverse mapping for the tags dictionary. This is needed to convert the predictions back to the tag names.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "        sentences = [['I', 'love', 'Python'], ['Python', 'is', 'great']]\n",
    "        tags = ['O', 'O', 'B']\n",
    "        word_dict, tag_dict, tag_dict_rev = create_corpus(sentences, tags)\n",
    "        # word_dict: {'I': 0, 'love': 1, 'Python': 2, 'is': 3, 'great': 4}\n",
    "        # tag_dict: {'O': 0, 'B': 1}\n",
    "        # tag_dict_rev: {0: 'O', 1: 'B'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict, tag_dict, tag_dict_rev = create_corpus(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Training Sequence List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:28<00:00, 116.81sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_list(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:25<00:00, 117.79sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_listC(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 1/0 2/0 3/0 4/0 5/0 6/1 7/0 8/0 9/0 10/0 11/0 12/1 13/0 14/0 9/0 15/0 1/0 16/2 17/0 18/0 19/0 20/0 21/0 \n",
      "U.N./B-geo relief/O coordinator/O Jan/B-per Egeland/I-per said/O Sunday/B-tim ,/O U.S./B-geo ,/O Indonesian/B-gpe and/O Australian/B-gpe military/O helicopters/O are/O ferrying/O out/O food/O and/O supplies/O to/O remote/O areas/O of/O western/O Aceh/B-geo province/O that/O ground/O crews/O can/O not/O reach/O ./O \n"
     ]
    }
   ],
   "source": [
    "print(train_seq[0])\n",
    "print(train_seq[0].to_words(sequence_list=train_seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ Default Features</h3>\n",
    "</div>\n",
    "\n",
    "To train the structured perceptron we must create a feature mapper and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[21] prev_tag:O::B-gpe\n",
      "[23] prev_tag:B-gpe::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[28] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[1] id:Thousands::O\n",
      "[2] id:of::O\n",
      "[4] id:demonstrators::O\n",
      "[5] id:have::O\n",
      "[6] id:marched::O\n",
      "[7] id:through::O\n",
      "[8] id:London::B-geo\n",
      "[10] id:to::O\n",
      "[12] id:protest::O\n",
      "[13] id:the::O\n",
      "[14] id:war::O\n",
      "[15] id:in::O\n",
      "[16] id:Iraq::B-geo\n",
      "[17] id:and::O\n",
      "[18] id:demand::O\n",
      "[13] id:the::O\n",
      "[19] id:withdrawal::O\n",
      "[2] id:of::O\n",
      "[20] id:British::B-gpe\n",
      "[22] id:troops::O\n",
      "[24] id:from::O\n",
      "[25] id:that::O\n",
      "[26] id:country::O\n",
      "[27] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper, train_seq[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp = StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "CPU times: user 4min 7s, sys: 1.77 s, total: 4min 9s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_c = structured_perceptron_c.StructuredPerceptronC(word_dict, tag_dict, feature_mapper)\n",
    "sp_c.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "CPU times: user 3min 59s, sys: 1.27 s, total: 4min\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp_c.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/nc/wbbw2w1x72dg89nnx4p_1xwr0000gn/T/ipykernel_49418/518294802.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fitted_models/01_SP_Default_Features\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0msp_c\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fitted_models/01C_SP_Default_Features\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "sp.save_model(\"fitted_models/01_SP_Default_Features\")\n",
    "sp_c.save_model(\"fitted_models/01C_SP_Default_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ New Features</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper_ext = ExtendedFeatures(train_seq)\n",
    "feature_mapper_ext.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[14] prev_tag:O::B-geo\n",
      "[16] prev_tag:B-geo::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[14] prev_tag:O::B-geo\n",
      "[16] prev_tag:B-geo::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[28] prev_tag:O::B-gpe\n",
      "[30] prev_tag:B-gpe::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[35] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[1, 2, 3] id:Thousands::O\n",
      "[1, 2, 3] firstupper::O\n",
      "[1, 2, 3] alphanum::O\n",
      "[4, 5, 3] id:of::O\n",
      "[4, 5, 3] lower::O\n",
      "[4, 5, 3] alphanum::O\n",
      "[7, 5, 3] id:demonstrators::O\n",
      "[7, 5, 3] lower::O\n",
      "[7, 5, 3] alphanum::O\n",
      "[8, 5, 3] id:have::O\n",
      "[8, 5, 3] lower::O\n",
      "[8, 5, 3] alphanum::O\n",
      "[9, 5, 3] id:marched::O\n",
      "[9, 5, 3] lower::O\n",
      "[9, 5, 3] alphanum::O\n",
      "[10, 5, 3] id:through::O\n",
      "[10, 5, 3] lower::O\n",
      "[10, 5, 3] alphanum::O\n",
      "[11, 12, 13] id:London::B-geo\n",
      "[11, 12, 13] firstupper::B-geo\n",
      "[11, 12, 13] alphanum::B-geo\n",
      "[15, 5, 3] id:to::O\n",
      "[15, 5, 3] lower::O\n",
      "[15, 5, 3] alphanum::O\n",
      "[17, 5, 3] id:protest::O\n",
      "[17, 5, 3] lower::O\n",
      "[17, 5, 3] alphanum::O\n",
      "[18, 5, 3] id:the::O\n",
      "[18, 5, 3] lower::O\n",
      "[18, 5, 3] alphanum::O\n",
      "[19, 5, 3] id:war::O\n",
      "[19, 5, 3] lower::O\n",
      "[19, 5, 3] alphanum::O\n",
      "[20, 5, 3] id:in::O\n",
      "[20, 5, 3] lower::O\n",
      "[20, 5, 3] alphanum::O\n",
      "[21, 12, 13] id:Iraq::B-geo\n",
      "[21, 12, 13] firstupper::B-geo\n",
      "[21, 12, 13] alphanum::B-geo\n",
      "[22, 5, 3] id:and::O\n",
      "[22, 5, 3] lower::O\n",
      "[22, 5, 3] alphanum::O\n",
      "[23, 5, 3] id:demand::O\n",
      "[23, 5, 3] lower::O\n",
      "[23, 5, 3] alphanum::O\n",
      "[18, 5, 3] id:the::O\n",
      "[18, 5, 3] lower::O\n",
      "[18, 5, 3] alphanum::O\n",
      "[24, 5, 3] id:withdrawal::O\n",
      "[24, 5, 3] lower::O\n",
      "[24, 5, 3] alphanum::O\n",
      "[4, 5, 3] id:of::O\n",
      "[4, 5, 3] lower::O\n",
      "[4, 5, 3] alphanum::O\n",
      "[25, 26, 27] id:British::B-gpe\n",
      "[25, 26, 27] firstupper::B-gpe\n",
      "[25, 26, 27] alphanum::B-gpe\n",
      "[29, 5, 3] id:troops::O\n",
      "[29, 5, 3] lower::O\n",
      "[29, 5, 3] alphanum::O\n",
      "[31, 5, 3] id:from::O\n",
      "[31, 5, 3] lower::O\n",
      "[31, 5, 3] alphanum::O\n",
      "[32, 5, 3] id:that::O\n",
      "[32, 5, 3] lower::O\n",
      "[32, 5, 3] alphanum::O\n",
      "[33, 5, 3] id:country::O\n",
      "[33, 5, 3] lower::O\n",
      "[33, 5, 3] alphanum::O\n",
      "[34] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper_ext, train_seq[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.929235\n",
      "Epoch: 1 Accuracy: 0.944526\n",
      "Epoch: 2 Accuracy: 0.948609\n",
      "Epoch: 3 Accuracy: 0.951267\n",
      "Epoch: 4 Accuracy: 0.953126\n",
      "Epoch: 5 Accuracy: 0.954476\n",
      "Epoch: 6 Accuracy: 0.955556\n",
      "Epoch: 7 Accuracy: 0.956719\n",
      "Epoch: 8 Accuracy: 0.957269\n",
      "Epoch: 9 Accuracy: 0.958295\n",
      "Epoch: 10 Accuracy: 0.958931\n",
      "Epoch: 11 Accuracy: 0.959925\n",
      "Epoch: 12 Accuracy: 0.960049\n",
      "Epoch: 13 Accuracy: 0.960416\n",
      "Epoch: 14 Accuracy: 0.961072\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "sp_ext = StructuredPerceptron(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext.num_epochs = 5\n",
    "sp_ext.fit(feature_mapper_ext.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_ext_c = structured_perceptron_c.StructuredPerceptron(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext_c.num_epochs = 5\n",
    "sp_ext_c.fit(feature_mapper_ext.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_ext.save_model(\"fitted_models/02_SP_Extended_Features\")\n",
    "sp_ext_c.save_model(\"fitted_models/02C_SP_Extended_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Learning NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 17:45:37.870907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connecting TPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        sentence_id          words tags\n0                 0      Thousands    O\n1                 0             of    O\n2                 0  demonstrators    O\n3                 0           have    O\n4                 0        marched    O\n...             ...            ...  ...\n839144        47957      officials    O\n839145        47957         within    O\n839146        47957            the    O\n839147        47957     government    O\n839148        47957              .    O\n\n[839149 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>words</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Thousands</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>demonstrators</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>have</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>marched</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>839144</th>\n      <td>47957</td>\n      <td>officials</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>839145</th>\n      <td>47957</td>\n      <td>within</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>839146</th>\n      <td>47957</td>\n      <td>the</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>839147</th>\n      <td>47957</td>\n      <td>government</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>839148</th>\n      <td>47957</td>\n      <td>.</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>839149 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_BERT = train\n",
    "MAX_LEN = 128\n",
    "train_BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Tokenizing and Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BERT[\"sentence_id\"] = train_BERT[\"sentence_id\"].fillna(method=\"ffill\")\n",
    "sentence = train_BERT.groupby(\"sentence_id\")[\"words\"].apply(list).values\n",
    "tag = train_BERT.groupby(by = 'sentence_id')['tags'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    df.loc[:, \"sentence_id\"] = df[\"sentence_id\"].fillna(method=\"ffill\")\n",
    "\n",
    "    enc_pos = preprocessing.LabelEncoder()\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "\n",
    "    df.loc[:, \"tags\"] = enc_tag.fit_transform(df[\"tags\"])\n",
    "\n",
    "    sentences = train_BERT.groupby(\"sentence_id\")[\"words\"].apply(list).values\n",
    "    tag = train_BERT.groupby(by = 'sentence_id')['tags'].apply(list).values\n",
    "    return sentences, tag, enc_pos, enc_tag\n",
    "\n",
    "sentences, tag, enc_pos, enc_tag = process_data(train_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3f8e467e9a9464686088937115894dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a18d19f670f14a6ea694f9754e34276f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7714f0a300c24be59460556d3c32cc2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fee74e5ed1c741a38bafd4e9cd64fe00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "def tokenize(data, max_len = MAX_LEN):\n",
    "    input_ids = list()\n",
    "    attention_mask = list()\n",
    "    for i in tqdm(range(len(data))):\n",
    "        # print(data[i])\n",
    "        encoded = tokenizer.encode_plus(data[i],\n",
    "                                        add_special_tokens = True,\n",
    "                                        max_length = MAX_LEN,\n",
    "                                        is_split_into_words=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        padding = 'max_length',\n",
    "                                        truncation=True,return_tensors = 'np')\n",
    "                        \n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_mask.append(encoded['attention_mask'])\n",
    "    return np.vstack(input_ids),np.vstack(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((34529,), (3837,), (34529,), (3837,))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(sentence,tag,random_state=42,test_size=0.1)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34529/34529 [00:07<00:00, 4422.08it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids,attention_mask = tokenize(X_train,max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3837/3837 [00:00<00:00, 4373.87it/s]\n"
     ]
    }
   ],
   "source": [
    "val_input_ids,val_attention_mask = tokenize(X_test,max_len = MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Padding and Truncation Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{128}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: Checking Padding and Truncation length's\n",
    "was = list()\n",
    "for i in range(len(input_ids)):\n",
    "    was.append(len(input_ids[i]))\n",
    "set(was)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{128}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Padding\n",
    "test_tag = list()\n",
    "for i in range(len(y_test)):\n",
    "    test_tag.append(np.array(y_test[i] + [0] * (128-len(y_test[i]))))\n",
    "    \n",
    "# TEST:  Checking Padding Length\n",
    "was = list()\n",
    "for i in range(len(test_tag)):\n",
    "    was.append(len(test_tag[i]))\n",
    "set(was)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{128}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Padding\n",
    "train_tag = list()\n",
    "for i in range(len(y_train)):\n",
    "    train_tag.append(np.array(y_train[i] + [0] * (128-len(y_train[i]))))\n",
    "    \n",
    "# TEST:  Checking Padding Length\n",
    "was = list()\n",
    "for i in range(len(train_tag)):\n",
    "    was.append(len(train_tag[i]))\n",
    "set(was)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT Model: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(bert_model,max_len = MAX_LEN):\n",
    "    input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
    "    attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
    "    bert_output = bert_model(input_ids,attention_mask = attention_masks,return_dict =True)\n",
    "    embedding = tf.keras.layers.Dropout(0.3)(bert_output[\"last_hidden_state\"])\n",
    "    output = tf.keras.layers.Dense(17,activation = 'softmax')(embedding)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = [output])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading tf_model.h5:   0%|          | 0.00/536M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f3fb22db713425db1a88e30d4bb7449"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 17:40:18.004851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/opt/anaconda3/envs/master/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    model = create_model(bert_model,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  2/576 [..............................] - ETA: 6:14:32 - loss: 3.1938 - accuracy: 0.0570"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(mode='min',patience=5)\n",
    "history_bert = model.fit([input_ids,attention_mask],\n",
    "                         np.array(train_tag),\n",
    "                         validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),\n",
    "                         epochs = 25,batch_size = 30*2,\n",
    "                         callbacks = early_stopping,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bert.history['accuracy'])\n",
    "plt.plot(history_bert.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bert.history['loss'])\n",
    "plt.plot(history_bert.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(val_input_ids,val_attention_mask):\n",
    "    return model.predict([val_input_ids,val_attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
    "    val_input = val_input_ids.reshape(1,128)\n",
    "    val_attention = val_attention_mask.reshape(1,128)\n",
    "    \n",
    "    # Print Original Sentence\n",
    "    sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
    "    print(\"Original Text : \",str(sentence))\n",
    "    print(\"\\n\")\n",
    "    true_enc_tag = enc_tag.inverse_transform(y_test)\n",
    "\n",
    "    print(\"Original Tags : \" ,str(true_enc_tag))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    pred_with_pad = np.argmax(pred(val_input,val_attention),axis = -1) \n",
    "    pred_without_pad = pred_with_pad[pred_with_pad>0]\n",
    "    pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
    "    print(\"Predicted Tags : \",pred_enc_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(val_input_ids[0],val_attention_mask[0],enc_tag,y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
