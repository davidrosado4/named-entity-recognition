{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Models\n",
    "<div style=\"color:red; font-size:14px;\">!! Don't define functions here, import them from utils.py</div>\n",
    "\n",
    "This notebook contains the code needed to train and store models to disk.\n",
    "\n",
    "Remember that if you use a function with a random state you have to fix it to a number so that the results are reproducible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n"
     ]
    }
   ],
   "source": [
    "# Cython import\n",
    "!python skseq/setup.py build_ext --build-lib=./skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from skseq.id_feature import IDFeatures\n",
    "from skseq.extended_feature import ExtendedFeatures\n",
    "\n",
    "from skseq import structured_perceptron_c \n",
    "from skseq.structured_perceptron import StructuredPerceptron\n",
    "\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 38366/38366 [01:50<00:00, 347.91sentence/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data_target_sets(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "We need to create our corpus using the training data. The corpus consists of two dictionaries, one for the words and one for the tags. The words dictionary maps each word to an index and the tags dictionary maps each tag to an index. We also need to create the reverse mapping for the tags dictionary. This is needed to convert the predictions back to the tag names.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "        sentences = [['I', 'love', 'Python'], ['Python', 'is', 'great']]\n",
    "        tags = ['O', 'O', 'B']\n",
    "        word_dict, tag_dict, tag_dict_rev = create_corpus(sentences, tags)\n",
    "        # word_dict: {'I': 0, 'love': 1, 'Python': 2, 'is': 3, 'great': 4}\n",
    "        # tag_dict: {'O': 0, 'B': 1}\n",
    "        # tag_dict_rev: {0: 'O', 1: 'B'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict, tag_dict, tag_dict_rev = create_corpus(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Training Sequence List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:28<00:00, 116.81sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_list(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding sequences: 100%|██████████| 38366/38366 [05:25<00:00, 117.79sequence/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_sequence_listC(word_dict, tag_dict, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 1/0 2/0 3/0 4/0 5/0 6/1 7/0 8/0 9/0 10/0 11/0 12/1 13/0 14/0 9/0 15/0 1/0 16/2 17/0 18/0 19/0 20/0 21/0 \n",
      "U.N./B-geo relief/O coordinator/O Jan/B-per Egeland/I-per said/O Sunday/B-tim ,/O U.S./B-geo ,/O Indonesian/B-gpe and/O Australian/B-gpe military/O helicopters/O are/O ferrying/O out/O food/O and/O supplies/O to/O remote/O areas/O of/O western/O Aceh/B-geo province/O that/O ground/O crews/O can/O not/O reach/O ./O \n"
     ]
    }
   ],
   "source": [
    "print(train_seq[0])\n",
    "print(train_seq[0].to_words(sequence_list=train_seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ Default Features</h3>\n",
    "</div>\n",
    "\n",
    "To train the structured perceptron we must create a feature mapper and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[9] prev_tag:O::B-geo\n",
      "[11] prev_tag:B-geo::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[21] prev_tag:O::B-gpe\n",
      "[23] prev_tag:B-gpe::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "[3] prev_tag:O::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[28] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[1] id:Thousands::O\n",
      "[2] id:of::O\n",
      "[4] id:demonstrators::O\n",
      "[5] id:have::O\n",
      "[6] id:marched::O\n",
      "[7] id:through::O\n",
      "[8] id:London::B-geo\n",
      "[10] id:to::O\n",
      "[12] id:protest::O\n",
      "[13] id:the::O\n",
      "[14] id:war::O\n",
      "[15] id:in::O\n",
      "[16] id:Iraq::B-geo\n",
      "[17] id:and::O\n",
      "[18] id:demand::O\n",
      "[13] id:the::O\n",
      "[19] id:withdrawal::O\n",
      "[2] id:of::O\n",
      "[20] id:British::B-gpe\n",
      "[22] id:troops::O\n",
      "[24] id:from::O\n",
      "[25] id:that::O\n",
      "[26] id:country::O\n",
      "[27] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper, train_seq[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp = StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "CPU times: user 4min 7s, sys: 1.77 s, total: 4min 9s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_c = structured_perceptron_c.StructuredPerceptronC(word_dict, tag_dict, feature_mapper)\n",
    "sp_c.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893815\n",
      "CPU times: user 3min 59s, sys: 1.27 s, total: 4min\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp_c.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nc/wbbw2w1x72dg89nnx4p_1xwr0000gn/T/ipykernel_49418/518294802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitted_models/01_SP_Default_Features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msp_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitted_models/01C_SP_Default_Features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "sp.save_model(\"fitted_models/01_SP_Default_Features\")\n",
    "sp_c.save_model(\"fitted_models/01C_SP_Default_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert\" style=\"padding: 20px;background-color: #2cbc84; color: white; margin-bottom: 15px;\">\n",
    "<h3>Structured Perceptron w/ New Features</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper_ext = ExtendedFeatures(train_seq)\n",
    "feature_mapper_ext.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "[0] init_tag:O\n",
      "\n",
      "\n",
      "Transition features\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[14] prev_tag:O::B-geo\n",
      "[16] prev_tag:B-geo::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[14] prev_tag:O::B-geo\n",
      "[16] prev_tag:B-geo::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[28] prev_tag:O::B-gpe\n",
      "[30] prev_tag:B-gpe::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "[6] prev_tag:O::O\n",
      "\n",
      "\n",
      "Final features\n",
      "[35] final_prev_tag:O\n",
      "\n",
      "\n",
      "Emission features\n",
      "[1, 2, 3] id:Thousands::O\n",
      "[1, 2, 3] firstupper::O\n",
      "[1, 2, 3] alphanum::O\n",
      "[4, 5, 3] id:of::O\n",
      "[4, 5, 3] lower::O\n",
      "[4, 5, 3] alphanum::O\n",
      "[7, 5, 3] id:demonstrators::O\n",
      "[7, 5, 3] lower::O\n",
      "[7, 5, 3] alphanum::O\n",
      "[8, 5, 3] id:have::O\n",
      "[8, 5, 3] lower::O\n",
      "[8, 5, 3] alphanum::O\n",
      "[9, 5, 3] id:marched::O\n",
      "[9, 5, 3] lower::O\n",
      "[9, 5, 3] alphanum::O\n",
      "[10, 5, 3] id:through::O\n",
      "[10, 5, 3] lower::O\n",
      "[10, 5, 3] alphanum::O\n",
      "[11, 12, 13] id:London::B-geo\n",
      "[11, 12, 13] firstupper::B-geo\n",
      "[11, 12, 13] alphanum::B-geo\n",
      "[15, 5, 3] id:to::O\n",
      "[15, 5, 3] lower::O\n",
      "[15, 5, 3] alphanum::O\n",
      "[17, 5, 3] id:protest::O\n",
      "[17, 5, 3] lower::O\n",
      "[17, 5, 3] alphanum::O\n",
      "[18, 5, 3] id:the::O\n",
      "[18, 5, 3] lower::O\n",
      "[18, 5, 3] alphanum::O\n",
      "[19, 5, 3] id:war::O\n",
      "[19, 5, 3] lower::O\n",
      "[19, 5, 3] alphanum::O\n",
      "[20, 5, 3] id:in::O\n",
      "[20, 5, 3] lower::O\n",
      "[20, 5, 3] alphanum::O\n",
      "[21, 12, 13] id:Iraq::B-geo\n",
      "[21, 12, 13] firstupper::B-geo\n",
      "[21, 12, 13] alphanum::B-geo\n",
      "[22, 5, 3] id:and::O\n",
      "[22, 5, 3] lower::O\n",
      "[22, 5, 3] alphanum::O\n",
      "[23, 5, 3] id:demand::O\n",
      "[23, 5, 3] lower::O\n",
      "[23, 5, 3] alphanum::O\n",
      "[18, 5, 3] id:the::O\n",
      "[18, 5, 3] lower::O\n",
      "[18, 5, 3] alphanum::O\n",
      "[24, 5, 3] id:withdrawal::O\n",
      "[24, 5, 3] lower::O\n",
      "[24, 5, 3] alphanum::O\n",
      "[4, 5, 3] id:of::O\n",
      "[4, 5, 3] lower::O\n",
      "[4, 5, 3] alphanum::O\n",
      "[25, 26, 27] id:British::B-gpe\n",
      "[25, 26, 27] firstupper::B-gpe\n",
      "[25, 26, 27] alphanum::B-gpe\n",
      "[29, 5, 3] id:troops::O\n",
      "[29, 5, 3] lower::O\n",
      "[29, 5, 3] alphanum::O\n",
      "[31, 5, 3] id:from::O\n",
      "[31, 5, 3] lower::O\n",
      "[31, 5, 3] alphanum::O\n",
      "[32, 5, 3] id:that::O\n",
      "[32, 5, 3] lower::O\n",
      "[32, 5, 3] alphanum::O\n",
      "[33, 5, 3] id:country::O\n",
      "[33, 5, 3] lower::O\n",
      "[33, 5, 3] alphanum::O\n",
      "[34] id:.::O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(feature_mapper_ext, train_seq[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### No Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.929235\n",
      "Epoch: 1 Accuracy: 0.944526\n",
      "Epoch: 2 Accuracy: 0.948609\n",
      "Epoch: 3 Accuracy: 0.951267\n",
      "Epoch: 4 Accuracy: 0.953126\n",
      "Epoch: 5 Accuracy: 0.954476\n",
      "Epoch: 6 Accuracy: 0.955556\n",
      "Epoch: 7 Accuracy: 0.956719\n",
      "Epoch: 8 Accuracy: 0.957269\n",
      "Epoch: 9 Accuracy: 0.958295\n",
      "Epoch: 10 Accuracy: 0.958931\n",
      "Epoch: 11 Accuracy: 0.959925\n",
      "Epoch: 12 Accuracy: 0.960049\n",
      "Epoch: 13 Accuracy: 0.960416\n",
      "Epoch: 14 Accuracy: 0.961072\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "sp_ext = StructuredPerceptron(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext.num_epochs = 5\n",
    "sp_ext.fit(feature_mapper_ext.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "sp_ext_c = structured_perceptron_c.StructuredPerceptron(word_dict, tag_dict, feature_mapper_ext)\n",
    "sp_ext_c.num_epochs = 5\n",
    "sp_ext_c.fit(feature_mapper_ext.dataset, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_ext.save_model(\"fitted_models/02_SP_Extended_Features\")\n",
    "sp_ext_c.save_model(\"fitted_models/02C_SP_Extended_Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Learning NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>of</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>have</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>marched</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839144</th>\n",
       "      <td>47957</td>\n",
       "      <td>officials</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839145</th>\n",
       "      <td>47957</td>\n",
       "      <td>within</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839146</th>\n",
       "      <td>47957</td>\n",
       "      <td>the</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839147</th>\n",
       "      <td>47957</td>\n",
       "      <td>government</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839148</th>\n",
       "      <td>47957</td>\n",
       "      <td>.</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839149 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id          words  tags\n",
       "0                 0      Thousands    16\n",
       "1                 0             of    16\n",
       "2                 0  demonstrators    16\n",
       "3                 0           have    16\n",
       "4                 0        marched    16\n",
       "...             ...            ...   ...\n",
       "839144        47957      officials    16\n",
       "839145        47957         within    16\n",
       "839146        47957            the    16\n",
       "839147        47957     government    16\n",
       "839148        47957              .    16\n",
       "\n",
       "[839149 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_BERT = train\n",
    "MAX_LEN = 128\n",
    "train_BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Tokenizing and Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BERT[\"sentence_id\"] = train_BERT[\"sentence_id\"].fillna(method=\"ffill\")\n",
    "sentence = train_BERT.groupby(\"sentence_id\")[\"words\"].apply(list).values\n",
    "tag = train_BERT.groupby(by = 'sentence_id')['tags'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goodie\\AppData\\Local\\Temp\\ipykernel_12244\\3195633987.py:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"tags\"] = enc_tag.fit_transform(df[\"tags\"])\n"
     ]
    }
   ],
   "source": [
    "def process_data(df):\n",
    "    df.loc[:, \"sentence_id\"] = df[\"sentence_id\"].fillna(method=\"ffill\")\n",
    "\n",
    "    enc_pos = preprocessing.LabelEncoder()\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "\n",
    "    df.loc[:, \"tags\"] = enc_tag.fit_transform(df[\"tags\"])\n",
    "\n",
    "    sentences = train_BERT.groupby(\"sentence_id\")[\"words\"].apply(list).values\n",
    "    tag = train_BERT.groupby(by = 'sentence_id')['tags'].apply(list).values\n",
    "    return sentences, tag, enc_pos, enc_tag\n",
    "\n",
    "sentences, tag, enc_pos, enc_tag = process_data(train_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "def tokenize(data, max_len = MAX_LEN):\n",
    "    input_ids = list()\n",
    "    attention_mask = list()\n",
    "    for i in tqdm(range(len(data))):\n",
    "        # print(data[i])\n",
    "        encoded = tokenizer.encode_plus(data[i],\n",
    "                                        add_special_tokens = True,\n",
    "                                        max_length = MAX_LEN,\n",
    "                                        is_split_into_words=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        padding = 'max_length',\n",
    "                                        truncation=True,return_tensors = 'np')\n",
    "                        \n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_mask.append(encoded['attention_mask'])\n",
    "    return np.vstack(input_ids),np.vstack(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34529,), (3837,), (34529,), (3837,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(sentence,tag,random_state=42,test_size=0.1)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34529/34529 [00:14<00:00, 2376.58it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids,attention_mask = tokenize(X_train,max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3837/3837 [00:01<00:00, 2572.40it/s]\n"
     ]
    }
   ],
   "source": [
    "val_input_ids,val_attention_mask = tokenize(X_test,max_len = MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Padding and Truncation Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{128}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: Checking Padding and Truncation length's\n",
    "was = list()\n",
    "for i in range(len(input_ids)):\n",
    "    was.append(len(input_ids[i]))\n",
    "set(was)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{128}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Padding\n",
    "test_tag = list()\n",
    "for i in range(len(y_test)):\n",
    "    test_tag.append(np.array(y_test[i] + [0] * (128-len(y_test[i]))))\n",
    "    \n",
    "# TEST:  Checking Padding Length\n",
    "was = list()\n",
    "for i in range(len(test_tag)):\n",
    "    was.append(len(test_tag[i]))\n",
    "set(was)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{128}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Padding\n",
    "train_tag = list()\n",
    "for i in range(len(y_train)):\n",
    "    train_tag.append(np.array(y_train[i] + [0] * (128-len(y_train[i]))))\n",
    "    \n",
    "# TEST:  Checking Padding Length\n",
    "was = list()\n",
    "for i in range(len(train_tag)):\n",
    "    was.append(len(train_tag[i]))\n",
    "set(was)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT Model: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(bert_model,max_len = MAX_LEN):\n",
    "    input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
    "    attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
    "    bert_output = bert_model(input_ids,attention_mask = attention_masks,return_dict =True)\n",
    "    embedding = tf.keras.layers.Dropout(0.3)(bert_output[\"last_hidden_state\"])\n",
    "    output = tf.keras.layers.Dense(17,activation = 'softmax')(embedding)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = [output])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 536M/536M [01:14<00:00, 7.23MB/s] \n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "c:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "model = create_model(bert_model,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  8/576 [..............................] - ETA: 5:28:16 - loss: 2.2548 - accuracy: 0.4170"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m,patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m history_bert \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit([input_ids,attention_mask],\n\u001b[0;32m      3\u001b[0m                          np\u001b[39m.\u001b[39;49marray(train_tag),\n\u001b[0;32m      4\u001b[0m                          validation_data \u001b[39m=\u001b[39;49m ([val_input_ids,val_attention_mask],np\u001b[39m.\u001b[39;49marray(test_tag)),\n\u001b[0;32m      5\u001b[0m                          epochs \u001b[39m=\u001b[39;49m \u001b[39m25\u001b[39;49m,batch_size \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m                          callbacks \u001b[39m=\u001b[39;49m early_stopping,verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Goodie\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(mode='min',patience=5)\n",
    "history_bert = model.fit([input_ids,attention_mask],\n",
    "                         np.array(train_tag),\n",
    "                         validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),\n",
    "                         epochs = 25,batch_size = 30*2,\n",
    "                         callbacks = early_stopping,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bert.history['accuracy'])\n",
    "plt.plot(history_bert.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bert.history['loss'])\n",
    "plt.plot(history_bert.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(val_input_ids,val_attention_mask):\n",
    "    return model.predict([val_input_ids,val_attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
    "    val_input = val_input_ids.reshape(1,128)\n",
    "    val_attention = val_attention_mask.reshape(1,128)\n",
    "    \n",
    "    # Print Original Sentence\n",
    "    sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
    "    print(\"Original Text : \",str(sentence))\n",
    "    print(\"\\n\")\n",
    "    true_enc_tag = enc_tag.inverse_transform(y_test)\n",
    "\n",
    "    print(\"Original Tags : \" ,str(true_enc_tag))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    pred_with_pad = np.argmax(pred(val_input,val_attention),axis = -1) \n",
    "    pred_without_pad = pred_with_pad[pred_with_pad>0]\n",
    "    pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
    "    print(\"Predicted Tags : \",pred_enc_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(val_input_ids[0],val_attention_mask[0],enc_tag,y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
